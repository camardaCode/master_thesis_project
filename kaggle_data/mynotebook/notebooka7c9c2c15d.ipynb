{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 8646276,
     "sourceType": "datasetVersion",
     "datasetId": 5068563
    },
    {
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 28083
    }
   ],
   "dockerImageVersionId": 30699,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import numpy as np \nimport pandas as pd \nimport os",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-06-25T20:26:27.516592Z",
     "iopub.execute_input": "2024-06-25T20:26:27.517445Z",
     "iopub.status.idle": "2024-06-25T20:26:28.4153Z",
     "shell.execute_reply.started": "2024-06-25T20:26:27.51741Z",
     "shell.execute_reply": "2024-06-25T20:26:28.414217Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "columns_names = ['Reports']\ndepositions_and_reports = pd.read_csv('/kaggle/input/depositionsandreports/InternetOfThingsCaseStudy_report.csv',sep=\"\\r\", names=columns_names)\ncrime_scene_reports = pd.read_csv('/kaggle/input/depositionsandreports/crime_scene_report.csv',sep=\"\\r\", names=columns_names)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T20:26:35.394022Z",
     "iopub.execute_input": "2024-06-25T20:26:35.394384Z",
     "iopub.status.idle": "2024-06-25T20:26:35.415022Z",
     "shell.execute_reply.started": "2024-06-25T20:26:35.394358Z",
     "shell.execute_reply": "2024-06-25T20:26:35.414032Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_output(messages):\n    prompt = pipeline.tokenizer.apply_chat_template(\n        messages, \n        tokenize=False, \n        add_generation_prompt=True\n    )\n\n    outputs = pipeline(\n        prompt,\n        max_new_tokens=1024,\n        eos_token_id=terminators,\n        do_sample=True,\n        temperature=0.6,\n        top_p=0.9,\n    )\n    print(outputs[0][\"generated_text\"][len(prompt):])\n\n\ndef chat_with_llama(dataset_column, object_location=False):\n    messages = []\n    \n    if object_location:\n        messages.append({\"role\": \"system\", \"content\": \"Translate the following sentence in Answer Set Programming facts. When answering, use predicates 'east(object1, object2)', 'west(object1, object2)', 'south(object1, object2)', 'north(object1, object2)', 'superior(object1, object2)', 'same_level(object1, object2)'. Produce as output just the translation and not the explanation. Example of sentence: 'The body was over the ground and, at the right of it, there was a knife'. Example of translation: superior(body, ground). east(body, knife). same_level(body, knife).  Sentence:\"})\n    else:\n        messages.append({\"role\": \"system\", \"content\": \"Translate the following sentence in Answer Set Programming facts. When answering, use predicates 'timestamp(year-month-day_hour:minute:second)', 'event(what_happened, year-month-day_hour , where)'. Produce as output just the translation and not the explanation. Example of sentence: '23:00 2024-02-09 The man laying on the sofa called his wife and wishes her a good birthday'. Example of translation: timestamp(2024-02-09_23:00).  event(the_man_called_his_wife, 2024-02-09_23:00, apartment). Sentence:\"})\n    \n    for report in dataset_column:\n        messages.append({\"role\": \"user\", \"content\": f\"{report}\"})\n        get_output(messages)\n        messages.pop()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-23T20:45:18.896111Z",
     "iopub.execute_input": "2024-05-23T20:45:18.897615Z",
     "iopub.status.idle": "2024-05-23T20:45:18.909509Z",
     "shell.execute_reply.started": "2024-05-23T20:45:18.89757Z",
     "shell.execute_reply": "2024-05-23T20:45:18.908138Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "terminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n    \nchat_with_llama(depositions_and_reports['Reports'])\nchat_with_llama(crime_scene_reports['Reports'], True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-23T20:45:22.76128Z",
     "iopub.execute_input": "2024-05-23T20:45:22.761671Z",
     "iopub.status.idle": "2024-05-23T20:48:34.64921Z",
     "shell.execute_reply.started": "2024-05-23T20:45:22.761642Z",
     "shell.execute_reply": "2024-05-23T20:48:34.646813Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
